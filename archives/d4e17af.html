<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-l.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-l.ico"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-l.ico"><link rel="mask-icon" href="/images/llogo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto Serif SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.min.css"><script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script><script src="https://code.jquery.com/jquery-3.4.1.min.js"></script><div id="append_parent"></div><div id="ajaxwaitid"></div><script type="text/javascript" src="/lib/activate-power-mode/activate-power-mode.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,document.body.addEventListener("input",POWERMODE)</script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"liuguangrui.top",root:"/",scheme:"Pisces",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!0},copycode:{enable:!0,show_result:!0,style:"mac"},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文以最基本的概念讲解生成对抗网络的设计原理，基于信息论与概率论分析生成对抗网络损失函数的推导过程。文章有助于理解不限于生成对抗网络的其它深度学习损失函数推演思路，作者致力于使读者可以举一反三构造符合特定环境的生成对抗网络。"><meta property="og:type" content="article"><meta property="og:title" content="生成对抗网络原理解析与损失函数推导"><meta property="og:url" content="https://liuguangrui.top/archives/d4e17af.html"><meta property="og:site_name" content="刘广睿的思维实验室"><meta property="og:description" content="本文以最基本的概念讲解生成对抗网络的设计原理，基于信息论与概率论分析生成对抗网络损失函数的推导过程。文章有助于理解不限于生成对抗网络的其它深度学习损失函数推演思路，作者致力于使读者可以举一反三构造符合特定环境的生成对抗网络。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://liuguangrui.top/images/20210524-1/1.webp"><meta property="og:image" content="https://liuguangrui.top/images/20210524-1/2.webp"><meta property="og:image" content="https://liuguangrui.top/images/20210524-1/3.webp"><meta property="og:image" content="https://liuguangrui.top/images/20210524-1/4.webp"><meta property="og:image" content="https://liuguangrui.top/images/20210524-1/5.webp"><meta property="article:published_time" content="2021-05-24T08:33:01.000Z"><meta property="article:modified_time" content="2021-06-04T14:16:39.066Z"><meta property="article:author" content="刘广睿"><meta property="article:tag" content="生成对抗网路"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://liuguangrui.top/images/20210524-1/1.webp"><link rel="canonical" href="https://liuguangrui.top/archives/d4e17af.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>生成对抗网络原理解析与损失函数推导 | 刘广睿的思维实验室</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style><link rel="alternate" href="/atom.xml" title="刘广睿的思维实验室" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><a target="_blank" rel="noopener" href="https://github.com/liuguangrui-hit" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;left:0;transform:scale(-1,1)" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">刘广睿的思维实验室</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">理论都是灰色的 唯有生命之树长青</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-top"><a href="/top/" rel="section"><i class="fa fa-signal fa-fw"></i>热榜</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-books"><a href="/books/" rel="section"><i class="fa fa-book fa-fw"></i>书房</a></li><li class="menu-item menu-item-music"><a href="/music/" rel="section"><i class="fa fa-music fa-fw"></i>乐室</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-comment fa-fw"></i>留言</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://liuguangrui.top/archives/d4e17af.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.webp"><meta itemprop="name" content="刘广睿"><meta itemprop="description" content="一名研究人工智能的存在主义者"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="刘广睿的思维实验室"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">生成对抗网络原理解析与损失函数推导</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表</span> <time title="创建时间：2021-05-24 16:33:01" itemprop="dateCreated datePublished" datetime="2021-05-24T16:33:01+08:00">2021-05-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新</span> <time title="修改时间：2021-06-04 22:16:39" itemprop="dateModified" datetime="2021-06-04T22:16:39+08:00">2021-06-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E7%A7%91%E7%A0%94%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">科研总结</span></a> </span></span><span id="/archives/d4e17af.html" class="post-meta-item leancloud_visitors" data-flag-title="生成对抗网络原理解析与损失函数推导" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">评论：</span> <a title="valine" href="/archives/d4e17af.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/archives/d4e17af.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>10k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>9 分钟</span></span><div class="post-description">本文以最基本的概念讲解生成对抗网络的设计原理，基于信息论与概率论分析生成对抗网络损失函数的推导过程。文章有助于理解不限于生成对抗网络的其它深度学习损失函数推演思路，作者致力于使读者可以举一反三构造符合特定环境的生成对抗网络。</div></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center"><i class="fa fa-quote-left"></i><p>本文以最基本的概念讲解生成对抗网络的设计原理，基于信息论与概率论分析生成对抗网络损失函数的推导过程。文章有助于理解不限于生成对抗网络的其它深度学习损失函数推演思路，作者致力于使读者可以举一反三构造符合特定环境的生成对抗网络。</p><i class="fa fa-quote-right"></i></blockquote><h1>1 机器学习模型</h1><p>机器学习模型可以实现如分类、聚类、回归、生成等许多功能，而要理解生成对抗网络（GAN）我们要先明白分类器和生成器的功能。</p><span class="label info">分类器（Classifier）</span> 分类器模型可以通过学习一些样本的类别信息，然后便能对输入的未知类别样本进行分类。比如让机器看一些不同类别的动物图片并告诉它图片对应的动物类别，然后机器便可自己辨别新的图片是哪个类别，这就是分类器。<br><span class="label info">生成器（Generatior）</span> 生成器模型可以通过学习一些样本，然后便能生成类似的样本。比如让机器看一些动物图片，然后机器便可自己生成动物图片，这就是生成器。<h1>2 生成对抗网络</h1><h1>2.1 古典生成器模型缺陷</h1><p>GAN是一种生成器，在它出现之前已经有了很多种类的生成器模型，比如玻尔兹曼机、变分自编码器等，但这些模型多使用生成数据和输入数据之间的MSE值（mean square error）来判断模型的好坏，其判断依据可以简单理解为生成数据与输入数据之间的距离，距离越小则认为效果越好。但在使用时人们逐渐发现这种判断方式对生成效果并不理想，尤其对于手写数字或小型图标等简单图像，MSE并不能有效反映生成图片的优劣。在图1中，我们认为左侧的数字“7”是好的生成图片，而右侧是差的生成图片，但两者MSE值相等。</p><p><img src="/images/loading.gif" data-original="/images/20210524-1/1.webp" alt="图1：左侧的数字“7”是好的生成图片，右侧是差的生成图片。"></p><h1>2.2 生成对抗网络原理</h1><p>2014年Ian Goodfellow提出了生成对抗网络模型，他从零和博弈中受到启发，令生成器（Generatior）与判别器（Discriminator）之间持续对抗，使两者能力不断增强。其中<span class="label info">判别器</span>是一种特殊的分类器，它是只有“是”或“否”两种输出的二元分类器。下面我们通过定性描述来直观理解一下GAN的构造。</p><p>如图2所示，GAN由三部分组成：<span class="label info">生成</span>、<span class="label info">判别</span>和<span class="label info">对抗</span>。生成和判别是两个独立的模块，分别由生成器与判别器实现。而对抗指的是生成器与判别器的交替训练过程。<br><img src="/images/loading.gif" data-original="/images/20210524-1/2.webp" alt="图2：GAN基本结构"></p><span class="label info">生成模块</span>：生成模块由生成器实现。负责依据随机向量产生样本，如图片、文字等。<br><span class="label info">判别模块</span>：判别模块由判别器实现。负责判断接收到的样本是真实样本还是生成器生成的样本。<br><span class="label info">对抗模块</span>：对抗模块构建生成模块与判别模块的关联，实现两者交替训练。<br><p>训练过程分为两步：</p><ul><li><span class="label success">Step1</span> 先让生成器产生一些假样本，和收集到的真实样本一起输入判别器，让判别器学习正确区分两者。</li><li><span class="label success">Step2</span> 当判别器能够熟练的判断现有数据后，再让生成器以生成能够欺骗判别器的假样本为目标，不断生成更好的假样本。<br></li></ul><p>重复进行<span class="label success">Step1</span> 和<span class="label success">Step2</span> 可使生成器与判别器相互促进，生成器的目的是让产生的样本逼近真实类别样本以假乱真，判别器的目的是将真实样本与生成器生成的样本加以区分。所以生成器产生的样本会越来越真实，而判别器的判别能力也会越来越强，最终达到纳什均衡。当判别器对任何样本的预测概率都接近0.5，即无法分辨样本是真实的还是生成器生成的，就停止训练。GAN的最终目标是生成一个足够好的生成器，其可以通过输入的随机向量，生成能够以假乱真的样本。</p><h1>3 信息论</h1><h2 id="3-1-信息量">3.1 信息量</h2><p>生成对抗网络要准确计算并生成以假乱真的样本，就需要用损失函数定量的分析生成器与判别器的性能优劣。机器学习的损失经常使用交叉熵来定义，生成对抗网络也同样使用交叉熵来定义损失。想理解生成对抗网络损失函数的本质，要从最基本的信息量讲起。信息量顾名思义即信息的数量，其大小通过事件发生概率的负对数衡量。为了便于理解，我们通过以下例子解释信息量的定义。假设我们听到了两件事，分别如下：</p><ul><li><span class="label success">事件A</span> 巴西队进入了2022年世界杯决赛。</li><li><span class="label success">事件B</span> 中国队进入了2022年世界杯决赛。</li></ul><p>凭直觉来说，很显然<span class="label success">事件B</span>的信息量比<span class="label success">事件A</span> 要大。究其原因，是因为<span class="label success">事件A</span>发生的概率很大，而<span class="label success">事件B</span>发生的概率很小。当越不可能的事情发生了，我们获取到的信息量就越大。越可能发生的事情发生了，我们获取到的信息量就越小。由此可见，信息量应该和事件发生的概率有关。</p><p>进一步的，必然事件是必然发生的，所以没有信息量。几乎不可能发生的事情一旦发生，则具有近乎无穷大的信息量。举例来说，<span class="label danger">我永远不会死</span>这句话信息量就近乎无穷，而<span class="label danger">我是我妈生的</span>这句话信息量就很低甚至为0。</p><p>通过以上的分析，可以得到信息量与概率的关系如图3所示，设$ X $是一个离散型随机变量，其取值集合为$ \chi $，概率分布函数$ p(x)=P_r(X=x),x \in \chi $，则定义事件$X = x_0$的信息量为：</p><p>\begin{equation}<br>I(x_0)=-log(p(x_0)) \label{amount_of_information}<br>\end{equation}</p><p>其中$ p(x_0) $的取值范围是$ [0,1] $。</p><p><img src="/images/loading.gif" data-original="/images/20210524-1/3.webp" alt="图3：信息量与概率的关系"></p><h2 id="3-2-熵">3.2 熵</h2><p>熵是一种用来描述事物内在混乱程度的度量，混乱程度越大则熵越大，混乱程度越小则熵越小。在信息论中熵可以表示一个事件的自信息量，即事件包含多少信息。比如对于一个必然事件，它的所有状态都是确定不变的，不会产生任何混乱或变化，那么它的熵就是最小值0。一般的，对于某个事件，其有$ n $种可能性，每一种可能性都有一个概率$ p(x_i) $，我们可以通过公式\eqref{amount_of_information}计算出某一种可能性的信息量，那么如何计算熵呢？下面我们来看一个更加典型的例子来进一步理解。</p><p>假设你拿出了你的电脑，按下开关，会有三种可能性，下表列出了每一种可能的概率机器对应的信息量。</p><table><thead><tr><th style="text-align:center">序号</th><th style="text-align:center">事件</th><th style="text-align:center">概率</th><th style="text-align:center">信息量</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">电脑正常开机</td><td style="text-align:center">0.7</td><td style="text-align:center">$ -log(p(A))=0.36 $</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">电脑无法开机</td><td style="text-align:center">0.2</td><td style="text-align:center">$ -log(p(B))=1.61 $</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">电脑爆炸了</td><td style="text-align:center">0.1</td><td style="text-align:center">$ -log(p(C))=2.30 $</td></tr></tbody></table><blockquote><p>注：文中的对数为均为自然对数</p></blockquote><p>我们有了所有可能性的信息量，而熵被定义为所有信息量的期望，以描述该事件的自信息量，即：</p><p>\begin{equation}<br>H(X)=- \sum_{i=1}^np(x_i)log(p(x_i)) \label{entropy}<br>\end{equation}</p><p>其中n代表所有的$ n $种可能性，所以上面问题的结果就是：</p><p>\begin{align}<br>\nonumber<br>H(X) &amp;= - \left [ p(A)log(p(A))+p(B)log(p(B))+p( C)log(p( C)) \right ]\\ \nonumber<br>&amp;= 0.7 \times 0.36 + 0.2 \times 1.61 +0.1 \times 2.30\\ \nonumber<br>&amp;= 0.804<br>\end{align}</p><p>值得一提的是，如投掷硬币这类比较特殊的问题，它只有两种可能，正面朝上或背面朝上，我们称之为0-1分布问题（二项分布的特例）。对于这类问题，熵的计算方法可以简化为如下算式：</p><p>\begin{align}<br>\nonumber<br>H(X) &amp;=- \sum_{i=1}^np(x_i)log(p(x_i)) \\<br>&amp;= -p(x)log(p(x))-(1-p(x))log(1-p(x))\\ \label{binomial_distribution_entropy}<br>\end{align}</p><h2 id="3-3-相对熵（KL散度）">3.3 相对熵（KL散度）</h2><p>相对熵又称KL散度，如果我们对于同一个随机变量x有两个单独的概率分布$ P(x) $和$ Q(x) $，我们可以使用KL散度来衡量这两个分布的差异。具体来说，相对熵就是用$ P $来描述目标问题相比用$ Q $来描述，得到的信息增量。</p><p>在机器学习中，$ P $往往用来表示样本的真实分布，比如$ [1,0,0] $表示当前样本属于第一类。$ Q $用来表示模型所预测的分布，比如$ [0.7,0.2,0.1]。 $<br>直观的理解就是如果我们用$ P $分布来描述样本，那么就非常完美。而用$ Q $分布来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和$ P $一样完美的描述。如果我们通过对模型的反复训练使$ Q $也能完美的描述样本，那么就不再需要额外的“信息增量”，$ Q $等价于$ P $。可以看出，减小$ P $与$ Q $之间的差距，即降低相对熵就是机器学习在训练模型时要达到的目标。</p><p>KL散度的公式推导基于使用不同的编码描述目标样本所产生的熵差，这里不详细推导，直接给出KL散度的计算公式：</p><p>\begin{equation}<br>D_{KL}(p||q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \label{kullback_leibler_divergence}<br>\end{equation}</p><p>其中$ n $为事件的所有可能性。$ D_{KL} $的值越小，表示$ q $分布与$ p $分布越接近。</p><h2 id="3-4-交叉熵">3.4 交叉熵</h2><p>对公式\eqref{kullback_leibler_divergence}变形可以得到：</p><p>\begin{align}<br>\nonumber<br>D_{KL}(p||q) &amp;= \sum_{i=1}^np(x_i)log(p(x_i)) - \sum_{i=1}^np(x_i)log(q(x_i))\\ \nonumber<br>&amp;= -H(p(x)) + - \left [ - \sum_{i=1}^np(x_i)log(q(x_i)) \right ]<br>\end{align}</p><p>等式的前一部分恰好是$ p $的熵，而将等式的后一部分定义为交叉熵：</p><p>\begin{equation}<br>H(p|q)=- \sum_{i=1}^np(x_i)log(q(x_i)) \label{cross_entropy}<br>\end{equation}</p><p>在机器学习中，为了评估样本真实类别(label)和模型预测类别(predicts)之间的差距，我们使用KL散度来判定模型的优劣，即$ D_{KL}(y||\hat y) $，由于KL散度中的前一部分$ −H(y) $是样本类别在真实世界中的熵，其固定不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中将模型的损失(loss)直接定义为交叉熵。这里的损失值的就是用模型预测分布描述样本相较于样本真实分布损失的信息量，优化损失函数（即交叉熵）使其最小化，便可令模型预测分布与样本真实分布尽可能的接近，使模型的预测能力增强。</p><p>特殊的，当随机$ P $和$ Q $是都二项分布时，即$ P $和$ Q $只有两个状态$ 0 $或$ 1 $，那么它们的差异可以用二进制交叉熵表示。令$ p $为$ P $的状态$ 1 $的概率，则$ (1-p) $是$ P $的状态$ 0 $的概率，同理，令$ q $为$ Q $的状态$ 1 $的概率，$ (1-q) $为$ Q $的状态$ 0 $的概率，则$ P $与$ Q $之间交叉熵为：</p><p>\begin{equation}<br>H(p|q)=-(plog(q)+(1-p)log(1-q)) \label{binomial_cross_entropy}<br>\end{equation}</p><h1>4 损失函数</h1><h2 id="4-1-分类模型损失函数">4.1 分类模型损失函数</h2><p>这里我们只讨论单分类问题，即每一个样本只能属于一个类别，比如一张图片只能是猫或者是狗，不能既有猫又有狗。那么此类问题的机器学习模型计算某一个样本的损失一般的方法是：</p><p>\begin{equation}<br>loss = - \sum_{i=1}^n y_ilog(\hat{y_i}) \label{loss}<br>\end{equation}</p><p>其中$ n $代表总类别数量。举例说明样本损失的计算方法，比如有如下样本：<br><img src="/images/loading.gif" data-original="/images/20210524-1/4.webp" alt="图4：猴子图片样本"><br>对应的标签和预测值如下表：</p><table><thead><tr><th style="text-align:center">*</th><th style="text-align:center">猫</th><th style="text-align:center">猴子</th><th style="text-align:center">老鼠</th></tr></thead><tbody><tr><td style="text-align:center">Label</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">Pred</td><td style="text-align:center">0.3</td><td style="text-align:center">0.6</td><td style="text-align:center">0.1</td></tr></tbody></table><p>那么模型对该样本损失为：</p><p>\begin{align}<br>\nonumber<br>loss &amp;= -(0 \times log(0.3) + 1 \times log(0.6) + 0 \times log(0.1))\\ \nonumber<br>&amp;= -log(0.6)<br>\end{align}</p><p>而对应一批样本（一个batch），则将其loss定义为其中所有样本loss的平均值（期望）：<br>\begin{equation}<br>batch\ loss = -\frac{1}{m} \sum_{j=1}^m \sum_{i=1}^n y_ilog(\hat{y_i}) \label{batch_loss}<br>\end{equation}<br>其中$ m $为当前batch中的样本数。</p><h2 id="4-2-GAN损失函数">4.2 GAN损失函数</h2><p>2014由Ian Goodfellow首次提出的GAN损失函数为：<br>\begin{equation}<br>\min_G \max_D V(D,G) = E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \label{GAN_loss}<br>\end{equation}</p><p>看到这一长串公式可能让人很难理解，下面我们将详细讲解其推导过程。</p><h3 id="4-2-1-判别器损失">4.2.1 判别器损失</h3><p>在GAN中，判别器（Discriminator）的优化目标是缩小判别器输出$ D(x) $的概率分布$ D $与样本真实标签概Label率分布$ L $的差异。其中$ L $与$ D $均为二项分布，$ L $概率分布只有两个取值100%或0%，即$ 1 $和$ 0 $；而$ D $概率分布表示判别器认为样本为真的概率，它的取值为100% ~ 0%，即$ [0,1] $。</p><p>那么，如果一个样本（一幅图片）$ x $是真实的（real），$ p $则为$ 1 $，$ q $为$ D(x_r) $，其交叉熵为：</p><p>\begin{align}<br>\nonumber<br>H(L|D_r) &amp;= -(plog(q)+(1-p)log(1-q))\\ \nonumber<br>&amp;= -(1 \times logD(x_r)+(1-1) \times log(1-D(x_r))\\<br>&amp;= -logD(x_r) \label{discriminator_real_entropy}<br>\end{align}</p><p>如果一个样本是生成器生成的，即为假的（fake），$ p $则为$ 0 $，$ q $为$ D(x_f) $，其交叉熵为：</p><p>\begin{align}<br>\nonumber<br>H(L|D_f) &amp;= -(plog(q)+(1-p)log(1-q))\\ \nonumber<br>&amp;= -(0 \times logD(x_f)+(1-0) \times log(1-D(x_f))\\<br>&amp;= -log(1-D(x_f)) \label{discriminator_fake_entropy}<br>\end{align}</p><p>于是，对于一个同时含有真实样本（real）与生成器生成样本（fake）的样本集，其交叉熵的平均值（期望）为：</p><p>\begin{equation}<br>H(L|D) = -\frac{1}{M}(\sum_{x_r}^M log(D(x_r)) + \sum_{x_f}^M log(1-D(x_f))) \label{discriminator_entropy}<br>\end{equation}</p><p>如上所述，判别器的优化目标是缩小$ D $与$ L $概率分布的差异，即最小化交叉熵。所以可以将判别器损失定义为</p><p>\begin{equation}<br>Loss_D = H(L|D) = -(\frac{1}{M}\sum_{x_r}^M log(D(x_r)) + \frac{1}{M}\sum_{x_f}^M log(1-D(x_f))) \label{discriminator_loss_1}<br>\end{equation}</p><p>为便于计算与表示，可将等式转化为如下的等价形式，即讲整体的交叉熵期望转换为对真假样本交叉熵各自期望的和。</p><p>\begin{equation}<br>Loss_D = E_{x_r} \left [ -log(D(x_r)) \right ] + E_{x_f} \left [ -log(1-D(x_f)) \right ] \label{discriminator_loss_2}<br>\end{equation}</p><p>其中$ x_f $特指生成器生成的假样本，一般使用$ G(z) $表示，$ z $为服从正太分布$ p(z) $的随机向量，而使用$ x $表示收集的样本集中的样本（图片）。则最终将判别器损失函数定义为：<br>\begin{equation}<br>Loss_D = - \lbrace E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \rbrace \label{discriminator_loss_3}<br>\end{equation}</p><h3 id="4-2-2-生成器损失">4.2.2 生成器损失</h3><p>生成器（Generator）的优化目标是让生成器的输出$ x_f $尽可能以假乱真，即判别器对生成器生成的样本判为$ 1 $（为真）的概率越高越好。</p><p>那么，如果一个样本$ x $是真实的（real），$ p $则为$ 1 $，$ q $为$ D(x_r) $，其交叉熵为：</p><p>\begin{align}<br>\nonumber<br>H(L|D_r) &amp;= -(plog(q)+(1-p)log(1-q))\\ \nonumber<br>&amp;= -(1 \times logD(x_r)+(1-1) \times log(1-D(x_r))\\<br>&amp;= -logD(x_r) \label{generator_real_entropy}<br>\end{align}</p><p>事实上，生成器G不关心真实样本（real）的情况，它只考虑其生成的$ x_f $是否可以欺骗判别器。从公式\eqref{generator_real_entropy}中也可以看出，在训练生成器时，对于真实样本的交叉熵$ -logD(x_r) $为固定值，并不会因生成器的优劣而变化。</p><p>如果一个样本是生成器生成的，即为假的（fake），$ p $则为$ 0 $，$ q $为$ D(x_f) $，其交叉熵为：</p><p>\begin{align}<br>\nonumber<br>H(L|D_f) &amp;= -(plog(q)+(1-p)log(1-q))\\ \nonumber<br>&amp;= -(0 \times logD(x_f)+(1-0) \times log(1-D(x_f))\\<br>&amp;= -log(1-D(x_f)) \label{generator_fake_entropy}<br>\end{align}</p><p>于是，对于一个同时含有真实样本（real）与生成器生成样本（fake）的样本集，其交叉熵的平均值（期望）为：</p><p>\begin{equation}<br>H(L|D) = -\frac{1}{M}(\sum_{x_r}^M log(D(x_r)) + \sum_{x_f}^M log(1-D(x_f))) \label{generator_entropy}<br>\end{equation}</p><p>如上所述，生成器的优化目标让其的输出$ x_f $尽可能以假乱真，交叉熵公式\eqref{generator_entropy}代表了判别器鉴别假样本的能力。对于一个确定的假样本，交叉熵越小，意味着判别器对假样本的判别结果概率分布与生成器生成的假样本概率分布差距越小。而在判别器模型固定的情况下，如果这个交叉熵越大，则说明生成器生成的样本可以欺骗判别器，达到以假乱真的效果。所以，生成器的优化目标可以描述为在判别器模型固定的情况下最大化交叉熵，进而将判别器损失定义为：</p><p>\begin{equation}<br>Loss_G = -H(L|D) = \frac{1}{M}\sum_{x_r}^M log(D(x_r)) + \frac{1}{M}\sum_{x_f}^M log(1-D(x_f)) \label{generator_loss_1}<br>\end{equation}</p><p>为便于计算与表示，可将等式转化为如下的等价形式，即讲整体的交叉熵期望转换为对真假样本交叉熵各自期望的和。</p><p>\begin{equation}<br>Loss_G = E_{x_r} \left [ -log(D(x_r)) \right ] + E_{x_f} \left [ -log(1-D(x_f)) \right ] \label{generator_loss_2}<br>\end{equation}</p><p>由于在训练生成器时，公式中$ E_{x_r} \left [ -log(D(x_r)) \right ] $为固定值，所以生成器损失可以简化为</p><p>\begin{equation}<br>Loss_G = E_{x_f} \left [ -log(1-D(x_f)) \right ] \label{generator_loss_3}<br>\end{equation}</p><p>其中$ x_f $特指生成器生成的假样本，一般使用$ G(z) $表示，$ z $为服从正太分布$ p(z) $的随机向量。则最终将生成器损失函数定义为：</p><p>\begin{equation}<br>Loss_G = E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \label{generator_loss_4}<br>\end{equation}</p><h3 id="4-2-3-整体损失函数表示">4.2.3 整体损失函数表示</h3><p>现在我们再来看Ian Goodfellow提出的GAN损失函数，就能比较好的理解了。</p><p>\begin{equation}<br>\min_G \max_D V(D,G) = E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \nonumber<br>\end{equation}</p><p>如2.2所述，在训练GAN的过程中，我们会迭代训练判别器与生成器。具体来说，先固定生成器不动，将判别器训练到一定程度；然后固定判别器，再进一步训练生成器；然后固定生成器，训练判别器……</p><p>对于判别器，在生成器固定不动的情况下，最大化优化函数：</p><p>\begin{equation}<br>\max_D V(D) = E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \nonumber<br>\end{equation}</p><p>其等价于优化（最小化）4.2.1中的判别器损失函数\eqref{discriminator_loss_3}：</p><p>\begin{equation}<br>Loss_D = - \lbrace E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \rbrace \nonumber<br>\end{equation}</p><p>对于生成器，在判别器固定不动的情况下，最小化优化函数：</p><p>\begin{equation}<br>\min_G V(G) = E_{x\sim p_{data}(x)} \left [ logD(x) \right ] + E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \nonumber<br>\end{equation}</p><p>其中等式的前一项$ E_{x\sim p_{data}(x)} \left [ logD(x) \right ] $不会随着生成器的训练发生变化，即为一个常量，则优化函数可简化为：</p><p>\begin{equation}<br>\min_G V(G) = E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \nonumber<br>\end{equation}</p><p>其等价于优化（最小化）4.2.2中的生成器损失函数\eqref{generator_loss_4}：<br>\begin{equation}<br>Loss_G = E_{z\sim p_z(z)} \left [ log(1-D(G(z))) \right ] \nonumber<br>\end{equation}</p><h1>5 生成对抗的本质作用</h1><p>相信通过上文你已经彻底理解了Ian Goodfellow提出的GAN原理与其损失函数定义。本节将解释GAN中生成器与判别器迭代训练时的具体关系，并说明生成器的本质作用。<br><img src="/images/loading.gif" data-original="/images/20210524-1/5.webp" alt="图5：GAN训练过程"><br>图5展示了GAN的训练过程，其中：</p><ul><li>蓝线：判别模型的分布$ D $（线中点为样本点，样本点越高，表示其为真实样本的概率越大）</li><li>黑虚线：真实数据分布P$ (x) $</li><li>绿实线：生成模型分布$ G $</li><li>$ x $线：真实样本数据</li><li>$ z $线：随机向量，一般为正太分布（G的输入）</li></ul><p>(a)为训练最初阶段。$ x $是真实样本数据，其始终服从黑虚线的概率分布（假设为正太分布）。随机向量$ z $通过生成器转变为样本。但由于初始状态的生成器效果差，无法有效拟合真实样本数据，即生成器模型分布（绿实现）的均值偏离真实数据分布（黑虚线）的均值。而初始状态的判别器效果同样较差，其分布有强烈波动（蓝线）。</p><p>(b)过程首先固定生成器，训练判别器，使判别器可以准确的判断真样本x和假样本$ G(z) $。可以看到图中判别器（蓝线）在生成器分布（绿线）均值处的值明显下降，即判断其为真实样本的概率下降。</p><p>(c) 过程固定判别器，训练生成器。生成器在得到判别器在原始G分布的均值处判真概率下降后，调整生成器的分布，使其均值避开判别模型分布概率较低的区域，进而使其分布逐渐靠近真实数据分布$ P(x) $。</p><p>循环(b)(c)，最终使生成模型分布G完全拟合真实数据分布$ P(x) $，在这种情况下判别器对真实样本和生成的假样本输出的判真概率相同，即无法辨别生成器产生的假样本，此时训练结束。</p><p>从结果上来看，生成器的作用是将随机向量$ z $映射到真实样本$ x $上。其通过不断与判别器对抗，尝试调整自身分布，最后将生成模型输出分布$ G $拟合为真实数据分布$ P(x) $。一般来讲，最后我们得到的生成器可以将符合某种分布的输入转换为目标分布的样本输出。</p></div><div><div><div style="text-align:center;color:#ccc;font-size:20px"><br>- - - - - - - - - - - - - - - - 本 文 结 束 啦 <i class="fa fa-star"></i> 感 谢 您 阅 读 - - - - - - - - - - - - - - - -</div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>刘广睿</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://liuguangrui.top/archives/d4e17af.html" title="生成对抗网络原理解析与损失函数推导">https://liuguangrui.top/archives/d4e17af.html</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E8%B7%AF/" rel="tag"># 生成对抗网路</a></div><div class="post-nav"><div class="post-nav-item"><a href="/archives/820ee20f.html" rel="prev" title="A fully scalable big data framework for Botnet detection based on network traffic analysis 研读报告"><i class="fa fa-chevron-left"></i> A fully scalable big data framework for Botnet detection based on network traffic analysis 研读报告</a></div><div class="post-nav-item"><a href="/archives/ba50aa7c.html" rel="next" title="20210415-2">20210415-2 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">1 机器学习模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">2 生成对抗网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">2.1 古典生成器模型缺陷</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">2.2 生成对抗网络原理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">3 信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E4%BF%A1%E6%81%AF%E9%87%8F"><span class="nav-text">3.1 信息量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E7%86%B5"><span class="nav-text">3.2 熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89"><span class="nav-text">3.3 相对熵（KL散度）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="nav-text">3.4 交叉熵</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">4 损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">4.1 分类模型损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-GAN%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">4.2 GAN损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-1-%E5%88%A4%E5%88%AB%E5%99%A8%E6%8D%9F%E5%A4%B1"><span class="nav-text">4.2.1 判别器损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-2-%E7%94%9F%E6%88%90%E5%99%A8%E6%8D%9F%E5%A4%B1"><span class="nav-text">4.2.2 生成器损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-3-%E6%95%B4%E4%BD%93%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%A1%A8%E7%A4%BA"><span class="nav-text">4.2.3 整体损失函数表示</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-text">5 生成对抗的本质作用</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="刘广睿" src="/images/avatar.webp"><p class="site-author-name" itemprop="name">刘广睿</p><div class="site-description" itemprop="description">一名研究人工智能的存在主义者</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">17</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">24</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/liuguangrui-hit" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liuguangrui-hit" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:liuguangrui@hit.edu.cn" title="E-Mail → mailto:liuguangrui@hit.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-history fa-" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list"><li><a href="/archives/ba50aa7c.html" title="20210415-2" target="_blank">20210415-2</a></li><li><a href="/archives/d4e17af.html" title="生成对抗网络原理解析与损失函数推导" target="_blank">生成对抗网络原理解析与损失函数推导</a></li><li><a href="/archives/820ee20f.html" title="A fully scalable big data framework for Botnet detection based on network traffic analysis 研读报告" target="_blank">A fully scalable big data framework for Botnet detection based on network traffic analysis 研读报告</a></li><li><a href="/archives/9863deb2.html" title="The Threat of Adversarial Attacks Against Machine Learning in Network Security: A Survey 研读报告" target="_blank">The Threat of Adversarial Attacks Against Machine Learning in Network Security: A Survey 研读报告</a></li><li><a href="/archives/92fdf3b8.html" title="Adversarial Attacks and Defenses in Intrusion Detection Systems: A Survey 研读报告" target="_blank">Adversarial Attacks and Defenses in Intrusion Detection Systems: A Survey 研读报告</a></li></ul></div><div><canvas id="canvas" style="width:60%">当前浏览器不支持canvas，请更换浏览器后再试</canvas></div><script>!function(){var o,a,r,f,t,i=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,0,0,0]]],e=document.getElementById("canvas");function h(t,e){for(var n=[1,2,3],l=["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"],o=0;o<i[e].length;o++)for(var a,h=0;h<i[e][o].length;h++){1==i[e][o][h]&&(a={x:14*(f+2)*t+2*h*(f+1)+(f+1),y:2*o*(f+1)+(f+1),stepX:Math.floor(4*Math.random()-2),stepY:-2*n[Math.floor(Math.random()*n.length)],color:l[Math.floor(Math.random()*l.length)],disY:1},r.push(a))}}function n(){e.height=100;for(var t=0;t<a.length;t++)!function(t,e){for(var n=0;n<i[e].length;n++)for(var l=0;l<i[e][n].length;l++)1==i[e][n][l]&&(o.beginPath(),o.arc(14*(f+2)*t+2*l*(f+1)+(f+1),2*n*(f+1)+(f+1),f,0,2*Math.PI),o.closePath(),o.fill())}(t,a[t]);for(t=0;t<r.length;t++)o.beginPath(),o.arc(r[t].x,r[t].y,f,0,2*Math.PI),o.fillStyle=r[t].color,o.closePath(),o.fill()}e.getContext&&(o=e.getContext("2d"),e.height=100,e.width=700,o.fillStyle="#f00",o.fillRect(10,10,50,50),a=[],r=[],f=e.height/20-1,t=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),a.push(t[1],t[2],10,t[3],t[4],10,t[5],t[6]),clearInterval(void 0),setInterval(function(){!function(){var t=[],e=/(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date),n=[];n.push(e[1],e[2],10,e[3],e[4],10,e[5],e[6]);for(var l=a.length-1;0<=l;l--)n[l]!==a[l]&&t.push(l+"_"+(Number(a[l])+1)%10);for(l=0;l<t.length;l++)h.apply(this,t[l].split("_"));a=n.concat()}(),function(){for(var t=0;t<r.length;t++)r[t].stepY+=r[t].disY,r[t].x+=r[t].stepX,r[t].y+=r[t].stepY,(r[t].x>700+f||r[t].y>100+f)&&(r.splice(t,1),t--)}(),n()},50))}()</script></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">刘广睿</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">59k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">54 分钟</span></div><script color="0,0,0" opacity="0.5" zindex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script></div></footer></div><script src="/lib/anime.min.js"></script><script src="https://code.jquery.com/jquery-3.4.1.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script defer src="/lib/three/three.min.js"></script><script defer src="/lib/three/three-waves.min.js"></script><script defer src="/lib/three/canvas_lines.min.js"></script><script defer src="/lib/three/canvas_sphere.min.js"></script><script src="/js/local-search.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script><script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script><script>window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://liuguangrui.top/archives/d4e17af.html',]
      });
      });</script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'RhffSWcBK1X6m1j5eAssOBU0-gzGzoHsz',
      appKey     : 'iA6xdICdlqAAkHmrDKAk5v4J',
      placeholder: "让精彩的思想如烟花般绽放！",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});</script><script>window.imageLazyLoadSetting={isSPA:!1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(n){n.imageLazyLoadSetting.processImages=i;var e=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var e=r[a],t=e,n=function(){r=r.filter(function(t){return e!==t})},i=new Image,o=t.getAttribute("data-original");i.onload=function(){t.src=o,n()},i.src=o}()}i(),n.addEventListener("scroll",function(){var t=i,e=n;clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this)</script></body></html>